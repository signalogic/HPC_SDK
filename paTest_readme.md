# paTest Demo

*** This demo is not yet available for download ***

After installing the [SigSRF SDK eval](https://github.com/signalogic/SigSRF_SDK), below are notes and example command lines for the paTest<sup> 1</sup> demo.  The demo has two (2) purposes:

 - show a predictive analytics application that applies algorithms and deep learning to continuous log data in order to predict failure anomalies
 
 - provide an example application, including Java and C/C++ source code, that interfaces to Spark and SigSRF software, and shows examples of API usage for both
 
<sup>1 </sup>paTest = predictive analytics test<br/>

# Other Demos

[mediaTest Demo (Streaming Media, Buffering, Transcoding, and Packet RFCs)](https://github.com/signalogic/SigSRF_SDK/blob/master/mediaTest_readme.md)

[iaTest Demo (Image Analytics)](https://github.com/signalogic/SigSRF_SDK/blob/master/iaTest_readme.md)

# Table of Contents

[Predictive Analytics from Log Data](#PredictiveAnalyticsLogData)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;[Data Flow Diagram](#DataFlowDiagram)<br/>
[Install Notes](#InstallNotes)<br/>
[Demo Notes](#DemoNotes)<br/>
[coCPU Notes](#coCPUNotes)<br/>

<a name="PredictiveAnalyticsLogData"></a>
# Predictive Analytics from Log Data

The demo uses actual logs generated by a high capacity telephony system (they've been scrubbed of proprietary information).  This system handles over 8000 simultaneous calls, with constant call setup and tear down, and with varying call duration, using Texas Instruments C6678 CPUs on an ATCA board.  A PowerPC CPU on the ATCA board handles call control and initiates sessions; the TI CPUs handle call data over 10 GbE network connections.

During capacity testing a highly infrequent and intermittent anomaly was found to occur, which required painstaking and time-consuming debug effort over the course of several weeks.  Eventually it turned out to be an error in DDR3 memory that required several conditions to precisely coincide.  This error is partially related to the "RowHammer" phenomenon that can occur with state-of-the-art DDR3 memories, in that it involved near simultaneous access of alternating rows in memory (i.e. adjacent rows of memory cells in the physical silicon layout).  To correct the error required slight -- but unforeseen -- changes to DDR3 memory chip setup (timing parameters).

One thing that stood out in the debug effort, is that most of the time spent was forcing the error to occur more often, thus making it "easier to see" and subject to a substantial increase in the rate of testing and software debug insertions.  Otherwise, the system could run for several days until the error manifested just once.  This is not an unusual situation for high capacity production systems  -- the last few problems tend to be the most difficult to isolate and identify.

Fortunately debug efforts were eventually successful, culminating in a short memory data test that could be inserted into the software at various points and would "catch" the error in action, adding an event to the log data.  This allowed the DDR3 chip timing parameter changes to be precisely calculated and then verified as the root cause resolution to the problem.

Afterwards, during debrief discussions, one common question among the engineers involved was whether deep learning methods might have been used to identify operational trends occcuring temporally near the anomaly, thus providing information about which "stress" conditions to emphasize to make the error appear more frequently.  If so, then weeks of engineering time could potentially be saved for future production systems with tough, intermittent issues.

It's interesting to note this is not a "glamorous" application of AI methods.  Instead it's a practical example of "useful AI", applied not as a cure-all or as press-worthy advance towards the "singularity", but as a powerful new tool in complement with traditional tools, such as detailed continuous log data and software debug.

<a name="DataflowDiagram"></a>
## DataFlow Diagram

Below is a data flow diagram showing I/O, algorithms, and convolutional neural networks used to predict anomalies in log data.

&nbsp;<br/>

![Image](https://github.com/signalogic/SigSRF_SDK/blob/master/images/Log_flow_diagram_algorithm_cnn.png?raw=true "Log data predictive analytics data flow diagram")

&nbsp;<br/>

As shown in the diagram, the approach centers around the idea of converting continuous log dato a series of images, which are then used to train a convolutional neural network.  A primary objective of this approach is take advantage of algorithms, training methods, and inference performance available due to computer vision current state-of-the-art.


<a name="InstallNotes"></a>
# Install Notes

TBD

<a name="DemoNotes"></a>
# Demo Notes

TBD

<a name="coCPUNotes"></a>
# coCPU&trade; Notes

As explained on the main SigSRF SDK page, the demos support coCPUâ„¢ technology, which adds NICs and up to 100s of coCPU cores to scale per-box streaming and performance density. For example, coCPUs can turn conventional 1U, 2U, and mini-ITX servers into high capacity media, HPC, and AI servers, or they can allow an embedded AI server to operate independently of the cloud. coCPU cards have NICs, allowing coCPUs to front streaming data and perform wirespeed packet filtering, routing decisions and processing.
The coCPU cards supported by the demos include:

* High performance, including extensive SIMD capability, 8 or more cores per CPU, L1 and L2 cache, and advanced DMA capability
* Contain onchip network I/O and packet processing and onchip PCIe
* Access to 2 (two) GB or more external DDR3 mem
* Able to efficiently decode camera input, e.g. H.264 streams arriving as input via onchip network I/O
* CGT<sup> 4</sup> supports gcc compatible C/C++ build and link, mature and reliable debug tools, RTOS, and numerous libraries

The current vision + AI server demo uses TI C6678 CPUs, which meet these requirements.  Over time, other suitable CPUs may become available.

Combining x86 and c66x CPUs and running software components necessary for AI applications such as H.264 decode, OpenCV and TensorFlow, is another form of an ["AI Accelerator"](https://en.wikipedia.org/wiki/AI_accelerator). The architecture described here favors fast, reliable development: mature semiconductors and tools, open source software, standard server format, and a wide range of easy-to-use peripherals and storage. 

<sup>4 </sup>CGT = Code Generation Tools
