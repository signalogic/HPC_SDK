# paTest Demo

*** This demo is not yet available for download ***

After installing the [SigSRF SDK eval](https://github.com/signalogic/SigSRF_SDK), below are notes and example command lines for the paTest<sup> 1</sup> demo.  The demo has two (2) purposes:

 - show a predictive analytics application that applies algorithms and deep learning to continuous log data in order to predict failure anomalies
 
 - provide an example application, including Java and C/C++ source code, that interfaces to Spark and SigSRF software, and shows examples of API usage for both
 
<sup>1 </sup>paTest = predictive analytics test<br/>

# Other Demos

[mediaTest Demo (Streaming Media, Buffering, Transcoding, and Packet RFCs)](https://github.com/signalogic/SigSRF_SDK/blob/master/mediaTest_readme.md)

[iaTest Demo (Image Analytics)](https://github.com/signalogic/SigSRF_SDK/blob/master/iaTest_readme.md)

# Table of Contents

[Predictive Analytics from Log Data](#PredictiveAnalyticsLogData)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;[Data Flow Diagram](#DataFlowDiagram)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;[Theory](#Theory)<br/>
[Install Notes](#InstallNotes)<br/>
[Demo Notes](#DemoNotes)<br/>
[coCPU Notes](#coCPUNotes)<br/>

<a name="PredictiveAnalyticsLogData"></a>
# Predictive Analytics from Log Data

The demo uses a real case as its basis, taking as input logs generated by a high capacity telephony system (they've been scrubbed of proprietary information).  This system handles over 8000 simultaneous calls, with constant call setup and tear down, and with varying call duration, using Texas Instruments C6678 CPUs on an ATCA board.  A PowerPC CPU on the ATCA board handles call control and initiates sessions; the TI CPUs handle call data over 10 GbE network connections.

During capacity testing a highly infrequent and intermittent anomaly was found to occur, which required painstaking and time-consuming debug effort over the course of several weeks.  Eventually it turned out (i)an error occurred in DDR3 memory that required several conditions to coincide, and (ii) the error is related to the ["RowHammer"](https://en.wikipedia.org/wiki/Row_hammer) phenomenon that can occur with state-of-the-art DDR3 memories (RowHammer involves near-simultaneous access of adjacent rows of memory cells in the chip's physical silicon layout).  To correct the error required slight -- but unforeseen -- changes to DDR3 memory chip setup (timing parameters).  This case was further remarkable in that all other, independent memory tests created for both the DDR3 chips and the board failed to induce the problem -- such was the uniqueness of the required combination of conditions, it only occurred during production stress testing.

In post-case analysis, one thing that stood out is that most of the debug time spent was forcing the error to occur more often, thus making it easier to see and subject to a substantial increase in the rate of testing and software debug insertions.  Otherwise, the system could run for several days until the error happened to occur in a way that affected system performance (typically manifesting as a random software critical error in the log data).  This is not an unusual situation for high capacity production systems  -- the last few problems tend to be the most difficult to isolate and identify (a colloquial expression for this situation is the ["90-90 rule"](https://en.wikipedia.org/wiki/Ninety-ninety_rule)).

Fortunately debug efforts were eventually successful, culminating in a "row marker" memory test that could be inserted into the software at various points in order to catch the error in action, adding a specific, precisely timed event to the log.  This allowed the related stress conditions to be correlated using log timestamps, the nature of the problem to be characterized, and finally, DDR3 chip timing parameter changes to be implemented and then verified as the root cause resolution to the problem.

During post-case debrief discussions, one common question among the engineers involved was whether deep learning methods might have been used to identify operational trends occcuring temporally near the anomaly, thus providing indications of which stress conditions to emphasize to make the error appear more frequently.  If so, then potentially weeks of engineering time could be saved for future production systems with tough, intermittent issues.

It's worthwhile to note this is not a "glamorous" application of AI methods.  Instead it's a practical example, applied not as a cure-all or as press-worthy advance towards the "singularity", but as a helpful new tool in complement with traditional tools, such as detailed continuous log data and software debug.

<a name="DataflowDiagram"></a>
## DataFlow Diagram

Below is a data flow diagram showing I/O, algorithms, and convolutional neural networks used to predict anomalies in log data.

&nbsp;<br/>

![Image](https://github.com/signalogic/SigSRF_SDK/blob/master/images/Log_flow_diagram_algorithm_cnn.png?raw=true "Log data predictive analytics data flow diagram")

&nbsp;<br/>

As shown in the above diagram, the approach centers around the concept of converting continuous log data a series of images, which are then used to train a convolutional neural network.  A primary objective of this approach is take advantage of algorithms, training methods, and inference performance available due to computer vision current state-of-the-art.

<a name="Theory"></a>
## Theory

Performing "recognition" based on frequency domain data is not a new approach, having been well-established as the primary basis for human vision and speech recognition.  For speech, the waveform displays below give an example.

![Image](https://github.com/signalogic/SigSRF_SDK/blob/master/images/spectrograph1.gif?raw=true "2-D spectrograph display of speech ime series data")

In the above diagram, the upper display shows time series data (in yellow), and the lower display shows its corresponding freuqency domain data as a "2-D spectrograph", with frequency on the y-axis, time on the x-axis, and amplitude as color coded.

The spectrograph display is actually a series of images, each representing about 20 msec of time series data.  For speech, 20 msec is the natural "framesize" of the underlying time series data produced by a human vocal tract.  For predictive analytics measurement, the natural framesize will vary depending on the specific system under test (SUT) and the nature of the data processed by the system.  In the case study being used for this demo, the natural framesize is about 100 usec.

Once the framesize is known, then a short-time FFT (Fourier analysis) can be performed to generate each image.  As noted in the above data flow diagram, overlap and windowing are used in the STFFT, in what is sometimes referred to as a "sliding FFT".

<a name="InstallNotes"></a>
# Install Notes

TBD

<a name="DemoNotes"></a>
# Demo Notes

TBD

<a name="coCPUNotes"></a>
# coCPU&trade; Notes

As explained on the main SigSRF SDK page, the demos support coCPUâ„¢ technology, which adds NICs and up to 100s of coCPU cores to scale per-box streaming and performance density. For example, coCPUs can turn conventional 1U, 2U, and mini-ITX servers into high capacity media, HPC, and AI servers, or they can allow an embedded AI server to operate independently of the cloud. coCPU cards have NICs, allowing coCPUs to front streaming data and perform wirespeed packet filtering, routing decisions and processing.
The coCPU cards supported by the demos include:

* High performance, including extensive SIMD capability, 8 or more cores per CPU, L1 and L2 cache, and advanced DMA capability
* Contain onchip network I/O and packet processing and onchip PCIe
* Access to 2 (two) GB or more external DDR3 mem
* Able to efficiently decode camera input, e.g. H.264 streams arriving as input via onchip network I/O
* CGT<sup> 4</sup> supports gcc compatible C/C++ build and link, mature and reliable debug tools, RTOS, and numerous libraries

The current vision + AI server demo uses TI C6678 CPUs, which meet these requirements.  Over time, other suitable CPUs may become available.

Combining x86 and c66x CPUs and running software components necessary for AI applications such as H.264 decode, OpenCV and TensorFlow, is another form of an ["AI Accelerator"](https://en.wikipedia.org/wiki/AI_accelerator). The architecture described here favors fast, reliable development: mature semiconductors and tools, open source software, standard server format, and a wide range of easy-to-use peripherals and storage. 

<sup>4 </sup>CGT = Code Generation Tools
